<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="3D-CAVLA is a vision-language-action model that integrates depth perception, chain-of-thought reasoning, and task-oriented pooling to improve 3D robotic manipulation and generalization to unseen tasks."/>
  <meta property="og:title" content="3D-CAVLA: Enhancing 3D Robotic Manipulation with Chain-of-Thought Reasoning and Depth Awareness"/>
  <meta property="og:description" content="Discover 3D-CAVLA, a model that advances 3D robotic manipulation by combining vision-language reasoning, depth features, and task-focused perception. Achieve higher success rates and stronger zero-shot generalization."/>
  <meta property="og:url" content="https://your-website-link-here"/>

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>3D-CAVLA: Leveraging Depth and 3D Context to Generalize Vision–Language Action Models for Unseen Tasks</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">3D-CAVLA: Leveraging Depth and 3D Context to Generalize Vision–Language
Action Models for Unseen Tasks</h1>
            
              <!-- Paper authors -->
             
<!--                 <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Second Author</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Third Author</a>
                  </span> -->
                  

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                <a target="_blank">Vineet Bhat</a>,</span>
                <span class="author-block">
                  <a target="_blank">Yu-Hsiang Lan</a>,</span>
                  <span class="author-block">
                    <a target="_blank">Prashanth Krishnamurthy</a>,</span>
                  <span class="author-block">
                    <a target="_blank">Ramesh Karri</a>,</span>                    
                  <span class="author-block">
                    <a target="_blank">Farshad Khorrami</a>,</span>                    
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">New York University</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://3d-cavla.github.io" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                      
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://3d-cavla.github.io" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
<!--                 <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Robotic manipulation in 3D requires learning an N degree-of-freedom joint space trajectory of a robot manipulator. Robots must possess semantic and visual perception abilities to transform real-world mappings of their workspace into the low-level control necessary for object manipulation. Recent work has demonstrated the capabilities of fine-tuning large Vision-Language Models (VLMs) to learn the mapping between RGB images, language instructions, and joint space control. These models typically take as input RGB images of the workspace and language instructions, and are trained on large datasets of teleoperated robot demonstrations. In this work, we explore methods to improve the scene context awareness of a popular recent Vision-Language-Action model by integrating chain-of-thought reasoning, depth perception, and task-oriented region of interest detection. Our experiments in the LIBERO simulation environment show that our proposed model, 3D-CAVLA, improves the success rate across various LIBERO task suites, achieving an average success rate of 98.1%. We also evaluate the zero-shot capabilities of our method, demonstrating that 3D scene awareness leads to robust learning and adaptation for completely unseen tasks. 3D-CAVLA achieves an absolute improvement of 8.8% on unseen tasks. We open-source our code and the unseen tasks dataset we created to promote community-driven research:
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->
  
<section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="title is-3">Architecture of 3D-CAVLA</h2>
        <img src="static/images/architecture.jpg" />
        <div class="content has-text-justified">
          <p>
            Our proposed model, SA-VLA, integrates <strong>chain-of-thought</strong> style narrative task descriptions, <strong>depth embeddings</strong> and <strong>Region Of
            Interest (ROI) pooling</strong> to improve the scene awareness of vision-language-action modeling. While GPT4 and ROI Detection are frozen
            components, our depth encoder is a lightweight PointNet inspired trainable network with spatial invariance transformation, convolution
            blocks and linear projections to project the embeddings to match the input dimensions of LLaMA2-7B.
          </p>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered is-flex-direction-column">
      <div class="column has-text-centered">
        <img src="static/images/tarp.jpg" />
        <p>Our Task-Aware Framework for Region of Interest Detection via Entity Recognition and Object Tracking.</p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
          <h2 class="title is-3">Experiments</h2>
          <h3 class="title is-4">Results on the LIBERO Benchmark</h3>
          <img src="static/images/libero_table_v2.jpg" />
          <div class="content has-text-justified">

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
              <tr>
                <p>
                  3D-CAVLA shows consistent improvement across all task suites in the dual camera setup. Most baselines overfit to the tasks and thus the margins are quite narrow. The strongest improvements are shown in long-horizon tasks (column 5) where chain-of-thought instructions helps the policy focus on one sub-task at a time. All scores are reported in success rate (%).
                </p>
              </tr>
            </table>
          </div>
          <br>
          <h3 class="title is-4">Performance on unseen tasks</h3>
          <div class="content has-text-justified">
            <img src="static/images/unseen_taskwise_table_v2.jpg" />
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
              <tr>
                <p>
                  Success-rate (in %) of OpenVLA-OFT and 3D-CAVLA on 10 unseen tasks. Both models cannot replicate performance on
                  seen tasks. 3D-CAVLA decomposes unseen tasks into seen steps and applies task-aware region-of-interest detection, enabling better
                  generalization.
                </p>
              </tr>
            </table>
            
          </div>
          <br>
          <h3 class="title is-4">Robot demonstrations on unseen tasks</h3>
          <div class="content has-text-justified">
            <img src="static/images/unseen_table_v2.jpg">
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
              <tr>
                <p>
                  Qualitative comparisons of OpenVLA-OFT and 3D-CAVLA on unseen LIBERO tasks. We show first, middle, and last frames of
                  each inference. The final two rows depict failures where both models misidentify target object or get distracted by previously seen objects.
                </p>
              </tr>
            </table>
          </div>
        <br>
      </div>
    </div>
</section>


<!-- Video -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
          <h3 class="title is-4">Rollout Videos: Qualitative Comparisons of OpenVLA-OFT and 3D-CAVLA on unseen LIBERO tasks</h3>
          <div class="columns is-vcentered interpolation-panel">
            <div class="column  has-text-centered">
              <h5><b>OpenVLA-OFT:</b><br>Put the chocolate pudding on the plate</h5>
              <h5 style="font-size:30px;">✅</h5>
              <video autoplay controls muted loop playsinline width="80%">
                <source src="static/videos/openvlaoft-task3-success.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column  has-text-centered">
              <h5><b>3D-CAVLA:</b><br>Put the chocolate pudding on the plate</h5>
              <h5 style="font-size:30px;">✅</h5>
              <video autoplay controls muted loop playsinline width="80%">
                <source src="static/videos/savla-task3-success.mp4" type="video/mp4">
              </video>
            </div>
          </div>
          <div class="columns is-vcentered interpolation-panel">
            <div class="column  has-text-centered">
              <h5><b>OpenVLA-OFT:</b><br>Place the white and yellow mug on the plate</h5>
              <h5 style="font-size:30px;">❌</h5>
              <video autoplay controls muted loop playsinline width="80%">
                <source src="static/videos/openvlaoft-task1-fail.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column  has-text-centered">
              <h5><b>3D-CAVLA:</b><br>Place the white and yellow mug on the plate</h5>
              <h5 style="font-size:30px;">✅</h5>
              <video autoplay controls muted loop playsinline width="80%">
                <source src="static/videos/savla-task1-success.mp4" type="video/mp4">
              </video>
            </div>
          </div>
          <div class="columns is-vcentered interpolation-panel">
            <div class="column  has-text-centered">
              <h5><b>OpenVLA-OFT:</b><br>Turn on the stove and put the bowl on it</h5>
              <h5 style="font-size:30px;">❌</h5>
              <video autoplay controls muted loop playsinline width="80%">
                <source src="static/videos/openvlaoft-task5-fail.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column  has-text-centered">
              <h5><b>3D-CAVLA:</b><br>Turn on the stove and put the bowl on it</h5>
              <h5 style="font-size:30px;">✅</h5>
              <video autoplay controls muted loop playsinline width="80%">
                <source src="static/videos/savla-task5-success.mp4" type="video/mp4">
              </video>
            </div>
          </div>
      </div>
    </div>
</section>
<!-- End video-->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
<!--       <pre><code>@misc{bhandari2025masalachailargescalespicenetlist,
      title={Masala-CHAI: A Large-Scale SPICE Netlist Dataset for Analog Circuits by Harnessing AI}, 
      author={Jitendra Bhandari and Vineet Bhat and Yuheng He and Hamed Rahmani and Siddharth Garg and Ramesh Karri},
      year={2025},
      eprint={2411.14299},
      archivePrefix={arXiv},
      primaryClass={cs.AR},
      url={https://arxiv.org/abs/2411.14299}, 
}</code></pre> -->
    </div>
</section>
<!--End BibTex citation -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p> Website borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> under a <a
                    href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0
                    International</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
